{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Project_def.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8a2Pv0cAqFn5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This project presents a study about how hyperparameters like the **number of convolutional layers** or the **kernel size** may affect the neural network performance. \n",
        "\n",
        "Regarding the number of layers, their relevance is something to take into account, since an oversized or undersized network will lead to overfitting or underfitting and high computing time.\n",
        "\n",
        "Kernel size has an important role as well.The kernel size will define the dimensions of the convolution result. Lowering this hyperparameter implies an improval on the computation time, but also provides a more basic generalization level.\n",
        "\n",
        "The study consists in analyzing the results of the training and evaluation of several architectures with the same dataset, each one different from the other. The experiment finishes with the conclusions obtained from this \n",
        "data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YTe1C5XDclhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook illustrates the use of convolutional networks for a multiclass classification over the MNIST dataset. This notebook is based on the PyTorch MNIST example: https://github.com/pytorch/examples/tree/master/mnist\n",
        "\n",
        "The MNIST dataset contains handwritten digits like these:\n",
        "\n",
        " ![MNIST samples](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "Each digit is 28x28 pixels and is labeled with the digit it contains, e.g. this is labeled with a \"0\":\n",
        "\n",
        "<img src=\"http://neuralnetworksanddeeplearning.com/images/mnist_complete_zero.png\" width=\"200\">\n"
      ]
    },
    {
      "metadata": {
        "id": "79J6f2D9rV6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Metholodogy\n",
        "\n",
        "First of all, the input of the network will be a 28x28 image corresponding to one digit. This image will pass through a convolutional layer with 20 square kernels, each one with size 5x5 and a stride equals to 1. \n",
        "\n",
        "After the convolution process, ReLU function is applied to the output. Then pooling is used to reduce the number of the network features and to ease the whole process reducing computation cost and time. \n",
        "\n",
        "Secondly, the last block is repeated, but its input will be the previous blockâ€™s output. The difference is the number kernels of the second layer, 50 in this case. \n",
        "\n",
        "The result of the previous operations is a 4x4 matrix with a 50 channels depth. This means that right now the number of parameters is 800. The usage of the two fully connected layers allows us to reduce this number to the 10 possible outputs.\n",
        "\n",
        "In order to evaluate this system with different hyperparameters, we have designed and trained 9 different networks and we have drawn conclusions based on the comparison of the results given, such as loss, accuracy and execution time.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "twEfnICvphKM",
        "colab_type": "code",
        "outputId": "e509c840-2dcc-464d-c0b3-305a4b1488f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pillow==5.4.1\n",
        "!pip install matplotlib \n",
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow==5.4.1 in /usr/local/lib/python3.6/dist-packages (5.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.20.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VrgRDACmp63p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oh9JxSFhsctd",
        "colab_type": "code",
        "outputId": "40dd24d5-aa62-49d2-e3c9-835bfa5a0b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f93cd4d94d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "938hLQzPqHRb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We create a class which extends nn.Module and takes a list of modules as input of the __init__() method to create a ModuleList. \n",
        "Forward function concatenates all previous layers and adds a softmax function at the end."
      ]
    },
    {
      "metadata": {
        "id": "FE8b0Fmxp7ii",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "      super(Net, self).__init__() \n",
        "      self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "      for _,l in enumerate(self.layers):\n",
        "        x = l(x)\n",
        "      return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ouXEO7kFqiDf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Flatten class to reshape the values and be able to append it to the list of modules."
      ]
    },
    {
      "metadata": {
        "id": "tzjo9RMyqEoq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "  def forward(self,x):\n",
        "    x = x.view(-1, x.size()[1]*x.size()[2]*x.size()[3] )\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WmTYDYqtA0d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Result class to store the data collected from training (model, validation loss, train loss, time)"
      ]
    },
    {
      "metadata": {
        "id": "KYeBR8IK9onf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Result():\n",
        "  def __init__(self, net, train_loss, val_loss, time):\n",
        "    self.net        = net\n",
        "    self.train_loss = train_loss\n",
        "    self.val_loss   = val_loss\n",
        "    self.time       = time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJU6nc6Nq58R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        start_time = time.time()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if batch_idx % log_interval == 0: \n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpGDB7EGq74Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(model, device, loader):\n",
        "    \n",
        "    model.eval()  # let's put the model in evaluation mode\n",
        "\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():  # we don't need gradient computation at all\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            validation_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    validation_loss /= len(loader.dataset)\n",
        "\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(loader.dataset),\n",
        "        100. * correct / len(loader.dataset)))\n",
        "    \n",
        "    return validation_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYo6HlYjspeO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0c958558-5519-47af-a486-72fece046cb0"
      },
      "cell_type": "code",
      "source": [
        "train_batch_size = 128\n",
        "\n",
        "mnist_mean = 0.1307\n",
        "mnist_stddev = 0.3081\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((mnist_mean,), (mnist_stddev,))\n",
        "                   ])),\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "valid_batch_size = 1000\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data',\n",
        "                   train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((mnist_mean,), (mnist_stddev,))\n",
        "                   ])),\n",
        "    batch_size=valid_batch_size,\n",
        "    shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eLxzkv0Lwvqb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nets = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qa7OAnfw1WAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net 0 is set as the default net with 2 convolution layers. The first one, 5x5 kernel size, stride = 1, no padding and 1 channel in to 20 out.\n",
        "The second one has a 5x5 kernel size, stride = 1, no padding and 20 channel in to 50 out."
      ]
    },
    {
      "metadata": {
        "id": "mdTnygUls19j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net0 = [nn.Conv2d(1,20,5,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(20,50,5,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(4*4*50,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Ml5ov0O2eB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net1 has 2 conv2d layers with 3x3 kernels size with the same stride as Net 0 (stride = 1) and adding padding = 1. Same number of channels in/out than Net0."
      ]
    },
    {
      "metadata": {
        "id": "zlR-EVOV2ddm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net1 = [nn.Conv2d(1,20,3,1,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(20,50,3,1,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(7*7*50,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E47noqq53A0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net2 has 2 conv2d layers. First kernel size of 5x5 and another one 3x3 kernel with the same stride as Net1 (stride = 1) and no padding = 0. Same number of channels in/out than Net0."
      ]
    },
    {
      "metadata": {
        "id": "vUaaOgnA3ApM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net2 = [nn.Conv2d(1,20,5,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(20,50,3,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(5*5*50,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7M2bXpOF54p1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net3 has 2 conv2d layers. The first kernel size of 3x3 with padding = 1 and another one 5x5 kernel with both stride = 1. Same number of channels in/out than Net0."
      ]
    },
    {
      "metadata": {
        "id": "_B18igMQ54g8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net3 = [nn.Conv2d(1,20,3,1,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(20,50,5,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(5*5*50,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q-sBuBh55JU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net4 has an only conv2d layer with a kernel size of 7x7 with stride = 3, no padding and 1 channel in, 20 out."
      ]
    },
    {
      "metadata": {
        "id": "_0LSxYUD4PC0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net4 = [nn.Conv2d(1,20,7,3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(4*4*20, 200),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(200,10)]\n",
        "nets.append(net4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxpEgEeSbGoe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net 5 has only one channel, a kernel size of 13x13 and a stride = 5. No padding. The reason of this parameters is trying to underfit the dataset."
      ]
    },
    {
      "metadata": {
        "id": "PO2SUdARbFZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " net5 = [nn.Conv2d(1,1,13,5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(2*2*1,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6cE1FVu6V2rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net 6 has a 5x5 kernel size with a stride = 3 in order to compare with the net above. No padding and a Maxpool."
      ]
    },
    {
      "metadata": {
        "id": "rWDLuLi6VbuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net6 = [nn.Conv2d(1,20,5,3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(4*4*20,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5oKb9wFQV0zx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net 7 has a 13x13 kernel size, stride = 3 and only 2 channel out to obtain an underfitting. No padding and one Maxpool."
      ]
    },
    {
      "metadata": {
        "id": "gw2SwoX-Vwzw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net7 = [nn.Conv2d(1,2,13,3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        Flatten(),\n",
        "        nn.Linear(3*3*2,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_IdJFRU8VfG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Net 8 has 5 conv2d layers. All set with a 3x3 kernel size, stride = 1 and no padding. There are 2 MaxPools in order to reduce computational cost."
      ]
    },
    {
      "metadata": {
        "id": "DsZ-LB8-8VIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net8 = [nn.Conv2d(1,20,3,1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(20,50,3,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(50,50,3,1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(50,50,3,1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(50,80,3,1),\n",
        "        nn.ReLU(),\n",
        "        Flatten(),\n",
        "        nn.Linear(2*2*80,500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500,10)]\n",
        "nets.append(net8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0xg11Kdu2Jm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train the different nets and we store all the results in order to plot them later."
      ]
    },
    {
      "metadata": {
        "id": "KIaJpPoNuTLS",
        "colab_type": "code",
        "outputId": "325fe8b3-f45a-49e6-bf53-d689c03be8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15623
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = {}\n",
        "for i,m in enumerate(nets):\n",
        "  print( \"------ Net \" + str(i) + \" ------\" )\n",
        "  start_time = time.time()\n",
        "  model = None\n",
        "  optimizer = None\n",
        "  model = Net(m).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  valid_x = []\n",
        "  num_epochs = 10\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "      start_time_epoch = time.time()\n",
        "      epoch_losses = train(80, model, device, train_loader, optimizer, epoch)\n",
        "      train_losses.extend(epoch_losses)\n",
        "      print(\"Training time: \" + str(time.time()-start_time_epoch))\n",
        "      valid_loss = validate(model, device, valid_loader)\n",
        "      valid_losses.append([valid_loss])\n",
        "      valid_x.append(len(train_losses) - 1)\n",
        "  \n",
        "  total_time = time.time()-start_time\n",
        "  results['net'+str(i)] = Result(model, train_losses, valid_losses, total_time)\n",
        "  \n",
        "  print(\"Total time: \" + str(total_time))\n",
        "  #plt.gcf().clear()\n",
        "  #plt.plot(train_losses, 'b-')\n",
        "  #plt.plot(valid_x, valid_losses, 'r-')\n",
        "  #plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ Net 0 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301021\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.603702\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.433208\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.459860\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.370031\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.221789\n",
            "Training time: 66.99132561683655\n",
            "\n",
            "Validation set: Average loss: 0.1676, Accuracy: 9504/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.208918\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.337949\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.139071\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.103127\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.127151\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.087134\n",
            "Training time: 66.75278329849243\n",
            "\n",
            "Validation set: Average loss: 0.0949, Accuracy: 9730/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.094088\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.067026\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.062841\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.044930\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.145065\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.046425\n",
            "Training time: 67.15778231620789\n",
            "\n",
            "Validation set: Average loss: 0.0655, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.024481\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.067695\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.102164\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.129062\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.043263\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.066797\n",
            "Training time: 67.01788592338562\n",
            "\n",
            "Validation set: Average loss: 0.0560, Accuracy: 9831/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.043891\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.054104\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.069711\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.008649\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.104127\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.036831\n",
            "Training time: 66.65008497238159\n",
            "\n",
            "Validation set: Average loss: 0.0457, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.128530\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.047741\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.014593\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.053475\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.078500\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.027997\n",
            "Training time: 66.7143669128418\n",
            "\n",
            "Validation set: Average loss: 0.0435, Accuracy: 9869/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.029570\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.073191\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.034139\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.049705\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.133208\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.039298\n",
            "Training time: 67.19571805000305\n",
            "\n",
            "Validation set: Average loss: 0.0390, Accuracy: 9890/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.037312\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.036562\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.022236\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.010459\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.073196\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.019275\n",
            "Training time: 66.98625874519348\n",
            "\n",
            "Validation set: Average loss: 0.0385, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.011708\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.018699\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.064864\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.038737\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.013485\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.017180\n",
            "Training time: 66.87825608253479\n",
            "\n",
            "Validation set: Average loss: 0.0363, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.003256\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.014394\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.017526\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.010287\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.057004\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.014913\n",
            "Training time: 66.93706607818604\n",
            "\n",
            "Validation set: Average loss: 0.0396, Accuracy: 9884/10000 (99%)\n",
            "\n",
            "Total time: 709.0290424823761\n",
            "------ Net 1 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309927\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.714559\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.519630\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.263399\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.360146\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.307995\n",
            "Training time: 81.64596009254456\n",
            "\n",
            "Validation set: Average loss: 0.1862, Accuracy: 9450/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.206163\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.108045\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.195693\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.131419\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.125291\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.133117\n",
            "Training time: 81.84857702255249\n",
            "\n",
            "Validation set: Average loss: 0.1017, Accuracy: 9692/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.131448\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.092792\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.072643\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.164201\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.078945\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.050043\n",
            "Training time: 81.75504922866821\n",
            "\n",
            "Validation set: Average loss: 0.0750, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.039798\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.056407\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.049597\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.055506\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.055739\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.053390\n",
            "Training time: 81.70716333389282\n",
            "\n",
            "Validation set: Average loss: 0.0658, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.081113\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.066406\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.064622\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.072416\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.025109\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.040053\n",
            "Training time: 82.15447521209717\n",
            "\n",
            "Validation set: Average loss: 0.0557, Accuracy: 9808/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.028989\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.093941\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.117597\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.041947\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.104572\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.052395\n",
            "Training time: 82.11358189582825\n",
            "\n",
            "Validation set: Average loss: 0.0509, Accuracy: 9831/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.034657\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.039031\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.022819\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.036076\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.054780\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.054147\n",
            "Training time: 81.77406787872314\n",
            "\n",
            "Validation set: Average loss: 0.0408, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.011209\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.066893\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.091119\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.036684\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.031644\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.031694\n",
            "Training time: 81.77000260353088\n",
            "\n",
            "Validation set: Average loss: 0.0401, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018463\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.055873\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.028817\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.025062\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.028516\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.104119\n",
            "Training time: 82.55086731910706\n",
            "\n",
            "Validation set: Average loss: 0.0347, Accuracy: 9873/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.036373\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.007993\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.064387\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.018315\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.032000\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.083282\n",
            "Training time: 81.87448811531067\n",
            "\n",
            "Validation set: Average loss: 0.0366, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Total time: 868.5447900295258\n",
            "------ Net 2 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313078\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.797247\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.349371\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.253445\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.262873\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.299128\n",
            "Training time: 61.132617235183716\n",
            "\n",
            "Validation set: Average loss: 0.1722, Accuracy: 9499/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.258714\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.189654\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.147431\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.135215\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.167006\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.111703\n",
            "Training time: 61.144540548324585\n",
            "\n",
            "Validation set: Average loss: 0.0966, Accuracy: 9712/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.080200\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.061752\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.123449\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.144850\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.070866\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.133143\n",
            "Training time: 61.67906975746155\n",
            "\n",
            "Validation set: Average loss: 0.0731, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.075581\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.075174\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.149924\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.082993\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.092025\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.180334\n",
            "Training time: 61.30237674713135\n",
            "\n",
            "Validation set: Average loss: 0.0541, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.098882\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.112585\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.025511\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.107312\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.072799\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.055685\n",
            "Training time: 61.04895091056824\n",
            "\n",
            "Validation set: Average loss: 0.0514, Accuracy: 9833/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.085803\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.039914\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.098357\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.126865\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.100948\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.026723\n",
            "Training time: 60.95077681541443\n",
            "\n",
            "Validation set: Average loss: 0.0519, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.035075\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.037510\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.053838\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.041151\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.054278\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.050544\n",
            "Training time: 60.93649339675903\n",
            "\n",
            "Validation set: Average loss: 0.0373, Accuracy: 9879/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.042950\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.020265\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.039561\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.023196\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.006933\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.106699\n",
            "Training time: 61.8233904838562\n",
            "\n",
            "Validation set: Average loss: 0.0375, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.063833\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.034750\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.020747\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.077753\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.039324\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.030203\n",
            "Training time: 61.39078140258789\n",
            "\n",
            "Validation set: Average loss: 0.0412, Accuracy: 9868/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.063085\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.019755\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.107868\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.029221\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.058897\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.019736\n",
            "Training time: 61.34174704551697\n",
            "\n",
            "Validation set: Average loss: 0.0354, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "Total time: 651.9426295757294\n",
            "------ Net 3 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305634\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.689232\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.337615\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.204810\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.256771\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.130096\n",
            "Training time: 85.34826374053955\n",
            "\n",
            "Validation set: Average loss: 0.1494, Accuracy: 9549/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.205357\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.143158\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.134718\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.113248\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.085342\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.071355\n",
            "Training time: 86.228511095047\n",
            "\n",
            "Validation set: Average loss: 0.0913, Accuracy: 9709/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.105859\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.022776\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.093923\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.042929\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.062039\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.080730\n",
            "Training time: 85.13744044303894\n",
            "\n",
            "Validation set: Average loss: 0.0713, Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.049998\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.053095\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.067131\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.036814\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.036500\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.099654\n",
            "Training time: 85.06922340393066\n",
            "\n",
            "Validation set: Average loss: 0.0564, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.051507\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.014660\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.044803\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.112261\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.113280\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.051120\n",
            "Training time: 85.3952407836914\n",
            "\n",
            "Validation set: Average loss: 0.0520, Accuracy: 9830/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.037547\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.058141\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.033776\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.024633\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.069178\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.025000\n",
            "Training time: 85.208087682724\n",
            "\n",
            "Validation set: Average loss: 0.0456, Accuracy: 9852/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.076771\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.036690\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.133246\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.043072\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.024121\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.099939\n",
            "Training time: 84.80779695510864\n",
            "\n",
            "Validation set: Average loss: 0.0465, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.082993\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.062034\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.033533\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.029863\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.132278\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.016277\n",
            "Training time: 84.90732336044312\n",
            "\n",
            "Validation set: Average loss: 0.0426, Accuracy: 9869/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.058695\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.049227\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.030004\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.007946\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.044587\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.084269\n",
            "Training time: 85.61837220191956\n",
            "\n",
            "Validation set: Average loss: 0.0434, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.050472\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.027459\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.038491\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.045507\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.036034\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.023470\n",
            "Training time: 84.98540782928467\n",
            "\n",
            "Validation set: Average loss: 0.0342, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Total time: 901.8696768283844\n",
            "------ Net 4 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333062\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.491691\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.574027\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.401937\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.329920\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.345980\n",
            "Training time: 15.232581377029419\n",
            "\n",
            "Validation set: Average loss: 0.2475, Accuracy: 9266/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.297027\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.243614\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.152926\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.125140\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.171482\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.228391\n",
            "Training time: 15.239705085754395\n",
            "\n",
            "Validation set: Average loss: 0.1593, Accuracy: 9512/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.188987\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.212991\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.172828\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.151464\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.124368\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.118026\n",
            "Training time: 15.195499897003174\n",
            "\n",
            "Validation set: Average loss: 0.1275, Accuracy: 9607/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.111631\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.125751\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.062493\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.099925\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.058419\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.098023\n",
            "Training time: 15.23120927810669\n",
            "\n",
            "Validation set: Average loss: 0.1131, Accuracy: 9636/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.091686\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.132686\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.164119\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.091121\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.111244\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.099464\n",
            "Training time: 15.235767364501953\n",
            "\n",
            "Validation set: Average loss: 0.0917, Accuracy: 9714/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.103189\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.098489\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.098085\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.111185\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.108771\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.051746\n",
            "Training time: 15.328646421432495\n",
            "\n",
            "Validation set: Average loss: 0.0830, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.058344\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.078730\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.075123\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.084985\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.105603\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.037553\n",
            "Training time: 15.267730474472046\n",
            "\n",
            "Validation set: Average loss: 0.0741, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.087368\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.145488\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.062051\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.093226\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.092897\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.052424\n",
            "Training time: 15.283018827438354\n",
            "\n",
            "Validation set: Average loss: 0.0710, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.081967\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.044315\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.093493\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.049226\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.019025\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.132670\n",
            "Training time: 15.810040950775146\n",
            "\n",
            "Validation set: Average loss: 0.0619, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.054046\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.022776\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.088934\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.033870\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.052765\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.020797\n",
            "Training time: 15.321519613265991\n",
            "\n",
            "Validation set: Average loss: 0.0604, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Total time: 168.13677144050598\n",
            "------ Net 5 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.320724\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.763171\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.560070\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.309119\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.517550\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.232534\n",
            "Training time: 13.809701681137085\n",
            "\n",
            "Validation set: Average loss: 1.2429, Accuracy: 5517/10000 (55%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.221414\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.264016\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.123120\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 1.236135\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 1.340142\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.143268\n",
            "Training time: 13.459879159927368\n",
            "\n",
            "Validation set: Average loss: 1.1303, Accuracy: 5998/10000 (60%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.255732\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.095067\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.145605\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 1.052261\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 1.019547\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.217842\n",
            "Training time: 13.460697650909424\n",
            "\n",
            "Validation set: Average loss: 1.1098, Accuracy: 6056/10000 (61%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.037608\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.964033\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 1.118980\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 1.150304\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 1.065392\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.132052\n",
            "Training time: 13.412791728973389\n",
            "\n",
            "Validation set: Average loss: 1.1118, Accuracy: 6156/10000 (62%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.037023\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 1.110695\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 1.058891\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.968288\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 1.146210\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.114851\n",
            "Training time: 13.485034942626953\n",
            "\n",
            "Validation set: Average loss: 1.1179, Accuracy: 6108/10000 (61%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.131666\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 1.040870\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 1.031712\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 1.222595\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 1.070207\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.194283\n",
            "Training time: 13.476844310760498\n",
            "\n",
            "Validation set: Average loss: 1.0974, Accuracy: 6299/10000 (63%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.996375\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 1.086783\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 1.056070\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.989088\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.990459\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.034900\n",
            "Training time: 13.485295295715332\n",
            "\n",
            "Validation set: Average loss: 1.0578, Accuracy: 6338/10000 (63%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.937442\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 1.016788\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.958337\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 1.083652\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 1.151261\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.089797\n",
            "Training time: 13.426107406616211\n",
            "\n",
            "Validation set: Average loss: 1.0190, Accuracy: 6478/10000 (65%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.018728\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 1.004742\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 1.090117\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 1.021465\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 1.269057\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.002193\n",
            "Training time: 13.488784790039062\n",
            "\n",
            "Validation set: Average loss: 1.0230, Accuracy: 6483/10000 (65%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.848375\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 1.098560\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 1.251538\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.858804\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 1.057461\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.125827\n",
            "Training time: 13.437209844589233\n",
            "\n",
            "Validation set: Average loss: 1.0127, Accuracy: 6401/10000 (64%)\n",
            "\n",
            "Total time: 148.22037768363953\n",
            "------ Net 6 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.286849\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.018560\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.546787\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.396177\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.242933\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.351781\n",
            "Training time: 15.800041675567627\n",
            "\n",
            "Validation set: Average loss: 0.2556, Accuracy: 9281/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.260340\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.149667\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.161892\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.235247\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.125075\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.187736\n",
            "Training time: 15.70111894607544\n",
            "\n",
            "Validation set: Average loss: 0.1818, Accuracy: 9485/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.158280\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.115231\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.212742\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.091709\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.096300\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.139008\n",
            "Training time: 15.692723035812378\n",
            "\n",
            "Validation set: Average loss: 0.1391, Accuracy: 9585/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.157325\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.150319\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.352757\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.101869\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.110577\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.110293\n",
            "Training time: 15.672785520553589\n",
            "\n",
            "Validation set: Average loss: 0.1236, Accuracy: 9629/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.124495\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.111841\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.138076\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.041146\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.107652\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.162913\n",
            "Training time: 15.657402038574219\n",
            "\n",
            "Validation set: Average loss: 0.1055, Accuracy: 9673/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.061979\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.087329\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.076737\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.046246\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.155601\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.079025\n",
            "Training time: 15.67146635055542\n",
            "\n",
            "Validation set: Average loss: 0.0962, Accuracy: 9697/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.065733\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.130661\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.068006\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.107807\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.108556\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.083283\n",
            "Training time: 15.669906854629517\n",
            "\n",
            "Validation set: Average loss: 0.0911, Accuracy: 9709/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044283\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.088385\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.068625\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.072341\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.080603\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.147618\n",
            "Training time: 15.739208459854126\n",
            "\n",
            "Validation set: Average loss: 0.0850, Accuracy: 9727/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.068511\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.049652\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.089810\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.055198\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.075793\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.084369\n",
            "Training time: 16.359520196914673\n",
            "\n",
            "Validation set: Average loss: 0.0776, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.087439\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.042701\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.036188\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.134171\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.065554\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.039207\n",
            "Training time: 16.02602505683899\n",
            "\n",
            "Validation set: Average loss: 0.0747, Accuracy: 9753/10000 (98%)\n",
            "\n",
            "Total time: 172.83531165122986\n",
            "------ Net 7 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.320617\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.796128\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.354355\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.581043\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.396561\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.456947\n",
            "Training time: 14.490269422531128\n",
            "\n",
            "Validation set: Average loss: 0.3351, Accuracy: 8951/10000 (90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.519693\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.288826\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.376690\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.183173\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.377129\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.371462\n",
            "Training time: 14.455561876296997\n",
            "\n",
            "Validation set: Average loss: 0.2935, Accuracy: 9055/10000 (91%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.439618\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.310434\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.234950\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.206532\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.373125\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.376839\n",
            "Training time: 14.469720840454102\n",
            "\n",
            "Validation set: Average loss: 0.2440, Accuracy: 9228/10000 (92%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.305162\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.283502\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.275112\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.267220\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.225654\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.252197\n",
            "Training time: 14.472095727920532\n",
            "\n",
            "Validation set: Average loss: 0.2457, Accuracy: 9227/10000 (92%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.381585\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.284206\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.209274\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.265833\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.319829\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.188577\n",
            "Training time: 14.492134094238281\n",
            "\n",
            "Validation set: Average loss: 0.2226, Accuracy: 9283/10000 (93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.246274\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.175990\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.244074\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.291846\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.129447\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.124200\n",
            "Training time: 14.453163146972656\n",
            "\n",
            "Validation set: Average loss: 0.2171, Accuracy: 9297/10000 (93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.202499\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.278130\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.174128\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.255668\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.223658\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.230204\n",
            "Training time: 14.512911558151245\n",
            "\n",
            "Validation set: Average loss: 0.1943, Accuracy: 9383/10000 (94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.251402\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.198687\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.297566\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.224078\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.220895\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.201582\n",
            "Training time: 14.441346168518066\n",
            "\n",
            "Validation set: Average loss: 0.1865, Accuracy: 9406/10000 (94%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.119727\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.105042\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.125419\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.254798\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.173462\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.231742\n",
            "Training time: 14.77475905418396\n",
            "\n",
            "Validation set: Average loss: 0.1923, Accuracy: 9386/10000 (94%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.165961\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.263110\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.296392\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.225615\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.254428\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.126084\n",
            "Training time: 14.507185459136963\n",
            "\n",
            "Validation set: Average loss: 0.1864, Accuracy: 9417/10000 (94%)\n",
            "\n",
            "Total time: 159.22188687324524\n",
            "------ Net 8 ------\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299483\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.295041\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.295686\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.291760\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.265254\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.654163\n",
            "Training time: 213.26431894302368\n",
            "\n",
            "Validation set: Average loss: 0.5616, Accuracy: 8079/10000 (81%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.596164\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.345872\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.327067\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.207157\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.204680\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.297436\n",
            "Training time: 214.3972408771515\n",
            "\n",
            "Validation set: Average loss: 0.1697, Accuracy: 9476/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.225499\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.127993\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.105194\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.180785\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.090992\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.140781\n",
            "Training time: 213.96213841438293\n",
            "\n",
            "Validation set: Average loss: 0.1007, Accuracy: 9680/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.083485\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.072335\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.102357\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.062611\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.081109\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.064732\n",
            "Training time: 215.3164050579071\n",
            "\n",
            "Validation set: Average loss: 0.0736, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.042863\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.100075\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.088996\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.024568\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.037024\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.060436\n",
            "Training time: 214.92696070671082\n",
            "\n",
            "Validation set: Average loss: 0.0855, Accuracy: 9721/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.077418\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.047514\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.058592\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.039703\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.089384\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.089611\n",
            "Training time: 214.01954627037048\n",
            "\n",
            "Validation set: Average loss: 0.0588, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.062477\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.026578\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.061397\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.067372\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.011872\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.099027\n",
            "Training time: 215.04833507537842\n",
            "\n",
            "Validation set: Average loss: 0.0564, Accuracy: 9809/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.061248\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.055769\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.018852\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.053701\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.061160\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.053759\n",
            "Training time: 214.9065237045288\n",
            "\n",
            "Validation set: Average loss: 0.0525, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.039911\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.098019\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.040073\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.038763\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.035760\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.011498\n",
            "Training time: 214.77349638938904\n",
            "\n",
            "Validation set: Average loss: 0.0451, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.106650\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.014814\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.009442\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.012740\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.028661\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.043649\n",
            "Training time: 214.14733362197876\n",
            "\n",
            "Validation set: Average loss: 0.0428, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Total time: 2247.9229934215546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qWJbAciIWhqM",
        "colab_type": "code",
        "outputId": "487f030c-e030-4033-c28c-a0727f164b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "plt.gcf().clear()\n",
        "times = {}\n",
        "for key in results:\n",
        "  plt.plot( results[key].val_loss, label=key )\n",
        "  plt.legend()\n",
        "  times[key]=results[key].time\n",
        "\n",
        "plt.show()\n",
        "print(\"\\n Execution times: \\n\")\n",
        "print(times)\n",
        "\n",
        "plt.bar(range(len(times)), list(times.values()))\n",
        "plt.xticks(range(len(times)), list(times.keys()))\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngUBgTAQRDgQCgUBgTAQRDgQCgUBgTPwflYHb8UQrJEsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Execution times: \n",
            "\n",
            "{'net0': 709.0290424823761, 'net1': 868.5447900295258, 'net2': 651.9426295757294, 'net3': 901.8696768283844, 'net4': 168.13677144050598, 'net5': 148.22037768363953, 'net6': 172.83531165122986, 'net7': 159.22188687324524, 'net8': 2247.9229934215546}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9l+tZsQzVBUqiAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DzemDzsfr7sB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "Firstly, the dependance on the dataset, more exactly, on the images size, and the number of possible labels was a limitation. The dataset used, MNIST, has 28x28 images. Each convolutional layer of the network reduces the size of its input, so we were not able to add as many convolutional layers as we intended to.\n",
        "\n",
        "The images provided by the dataset are very simple. They only have dark colors and ten possible outputs, so the usage of really complex network architectures did not make sense in this experiment.\n",
        "\n",
        "However, the results of training the different models lead us to conclude the following:\n",
        "\n",
        "\n",
        "1.   When the **number of layers** is big, the training and validation time is exponentially increased, nevertheless, the accuracy does not increase as much as what we would consider worth.\n",
        "2.   The **kernel size** is also directly related to the time, but not the accuracy. We observed that even though time highered, it costed less than adding lots of layers. \n",
        "3. The tests performed on the **stride** were not conclusive due to the simplicity of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}